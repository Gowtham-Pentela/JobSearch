{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in /Users/gowthampentela/Library/Python/3.13/lib/python/site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->webdriver-manager) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->webdriver-manager) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->webdriver-manager) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->webdriver-manager) (2025.1.31)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (4.13.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from beautifulsoup4) (4.12.2)\n",
      "Collecting schedule\n",
      "  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Downloading schedule-1.2.2-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: schedule\n",
      "Successfully installed schedule-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install webdriver-manager\n",
    "!pip3 install beautifulsoup4\n",
    "\n",
    "!pip3 install schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mchromedriver\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "old_path = ChromeDriverManager().install()\n",
    "!mv \"{old_path}\" .\n",
    "!ls | grep chromedriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping new jobs...\n",
      "Scraping completed at 2025-03-17 17:53:21. Data saved.\n",
      "Scraping new jobs...\n",
      "Scraping completed at 2025-03-17 17:53:36. Data saved.\n",
      "Scraping new jobs...\n",
      "Scraping completed at 2025-03-17 17:53:50. Data saved.\n",
      "Scraping new jobs...\n",
      "Scraping completed at 2025-03-17 17:54:04. Data saved.\n",
      "Scraping new jobs...\n",
      "Scraping completed at 2025-03-17 17:54:19. Data saved.\n",
      "Scraping new jobs...\n",
      "Scraping completed at 2025-03-17 17:54:33. Data saved.\n",
      "Scraping new jobs...\n",
      "Scraping completed at 2025-03-17 17:54:48. Data saved.\n",
      "Scraping new jobs...\n",
      "Scraping completed at 2025-03-17 17:55:02. Data saved.\n",
      "Scraping new jobs...\n",
      "Scraping completed at 2025-03-17 17:55:17. Data saved.\n",
      "Scraping new jobs...\n",
      "Scraping completed at 2025-03-17 17:55:31. Data saved.\n",
      "Scraping new jobs...\n",
      "Scraping completed at 2025-03-17 17:55:46. Data saved.\n",
      "Scraping new jobs...\n",
      "Scraping completed at 2025-03-17 17:56:00. Data saved.\n",
      "Scraping new jobs...\n",
      "Scraping completed at 2025-03-17 17:56:14. Data saved.\n",
      "Scraping new jobs...\n",
      "Scraping completed at 2025-03-17 17:56:29. Data saved.\n",
      "Scraping new jobs...\n",
      "Scraping completed at 2025-03-17 17:56:43. Data saved.\n",
      "Scraping new jobs...\n",
      "Scraping completed at 2025-03-17 17:56:58. Data saved.\n",
      "Scraping new jobs...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;66;03m#time.sleep(1800)  # Run every 30 minutes\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 86\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping new jobs...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m     jobs \u001b[38;5;241m=\u001b[39m scrape_indeed() \u001b[38;5;241m+\u001b[39m \u001b[43mscrape_linkedin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     save_to_csv(jobs)\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping completed at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Data saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 53\u001b[0m, in \u001b[0;36mscrape_linkedin\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m driver \u001b[38;5;241m=\u001b[39m setup_driver()\n\u001b[1;32m     51\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.linkedin.com/jobs/search/?keywords=Data\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m20Science&location=United\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m20States\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(driver\u001b[38;5;241m.\u001b[39mpage_source, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m driver\u001b[38;5;241m.\u001b[39mquit()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Setup WebDriver\n",
    "def setup_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in background\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Function to scrape Indeed\n",
    "def scrape_indeed():\n",
    "    driver = setup_driver()\n",
    "    driver.get(\"https://www.indeed.com/jobs?q=Data+Science&l=United+States\")\n",
    "\n",
    "    time.sleep(5)  # Wait for page to load\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()  # Close driver after fetching page source\n",
    "\n",
    "    jobs = []\n",
    "    for job_card in soup.find_all(\"div\", class_=\"job_seen_beacon\"):\n",
    "        title = job_card.find(\"h2\").text.strip() if job_card.find(\"h2\") else \"N/A\"\n",
    "        company = job_card.find(\"span\", class_=\"companyName\").text.strip() if job_card.find(\"span\", class_=\"companyName\") else \"N/A\"\n",
    "        location = job_card.find(\"div\", class_=\"companyLocation\").text.strip() if job_card.find(\"div\", class_=\"companyLocation\") else \"N/A\"\n",
    "        link = \"https://www.indeed.com\" + job_card.find(\"a\")[\"href\"] if job_card.find(\"a\") else \"N/A\"\n",
    "\n",
    "        jobs.append({\n",
    "            \"Source\": \"Indeed\",\n",
    "            \"Title\": title,\n",
    "            \"Company\": company,\n",
    "            \"Location\": location,\n",
    "            \"URL\": link,\n",
    "            \"Date Scraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "    return jobs\n",
    "\n",
    "# Function to scrape LinkedIn (dynamic approach)\n",
    "def scrape_linkedin():\n",
    "    driver = setup_driver()\n",
    "    driver.get(\"https://www.linkedin.com/jobs/search/?keywords=Data%20Science&location=United%20States\")\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    jobs = []\n",
    "    for job_card in soup.find_all(\"div\", class_=\"base-card\"):\n",
    "        title = job_card.find(\"h3\", class_=\"base-search-card__title\").text.strip() if job_card.find(\"h3\", class_=\"base-search-card__title\") else \"N/A\"\n",
    "        company = job_card.find(\"h4\", class_=\"base-search-card__subtitle\").text.strip() if job_card.find(\"h4\", class_=\"base-search-card__subtitle\") else \"N/A\"\n",
    "        location = job_card.find(\"span\", class_=\"job-search-card__location\").text.strip() if job_card.find(\"span\", class_=\"job-search-card__location\") else \"N/A\"\n",
    "        link = job_card.find(\"a\")[\"href\"] if job_card.find(\"a\") else \"N/A\"\n",
    "\n",
    "        jobs.append({\n",
    "            \"Source\": \"LinkedIn\",\n",
    "            \"Title\": title,\n",
    "            \"Company\": company,\n",
    "            \"Location\": location,\n",
    "            \"URL\": link,\n",
    "            \"Date Scraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "    return jobs\n",
    "\n",
    "# Function to save jobs to CSV\n",
    "def save_to_csv(jobs, filename=\"job_listings.csv\"):\n",
    "    df = pd.DataFrame(jobs)\n",
    "    df.to_csv(filename, mode='a', index=False, header=not pd.io.common.file_exists(filename))\n",
    "\n",
    "# Main automation loop\n",
    "def main():\n",
    "    while True:\n",
    "        print(\"Scraping new jobs...\")\n",
    "\n",
    "        jobs = scrape_indeed() + scrape_linkedin()\n",
    "        save_to_csv(jobs)\n",
    "\n",
    "        print(f\"Scraping completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}. Data saved.\")\n",
    "\n",
    "        #time.sleep(1800)  # Run every 30 minutes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping new jobs...\n",
      "Scraping completed at 2025-03-18 13:21:07. Data saved.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Setup WebDriver\n",
    "def setup_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in background\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Function to scrape Indeed\n",
    "def scrape_indeed():\n",
    "    driver = setup_driver()\n",
    "    driver.get(\"https://www.indeed.com/jobs?q=Data+Science&l=United+States\")\n",
    "\n",
    "    time.sleep(5)  # Allow time for page load\n",
    "\n",
    "    # Scroll to load more jobs\n",
    "    for _ in range(3):\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(2)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    jobs = []\n",
    "    cutoff_time = datetime.now() - timedelta(days=1)  # 24-hour filter\n",
    "\n",
    "    for job_card in soup.find_all(\"div\", class_=\"job_seen_beacon\"):\n",
    "        title = job_card.find(\"h2\").text.strip() if job_card.find(\"h2\") else \"N/A\"\n",
    "        company = job_card.find(\"span\", class_=\"companyName\").text.strip() if job_card.find(\"span\", class_=\"companyName\") else \"N/A\"\n",
    "        location = job_card.find(\"div\", class_=\"companyLocation\").text.strip() if job_card.find(\"div\", class_=\"companyLocation\") else \"N/A\"\n",
    "        link = \"https://www.indeed.com\" + job_card.find(\"a\")[\"href\"] if job_card.find(\"a\") else \"N/A\"\n",
    "        posted_text = job_card.find(\"span\", class_=\"date\").text.strip() if job_card.find(\"span\", class_=\"date\") else \"N/A\"\n",
    "\n",
    "        # Convert posted time to datetime\n",
    "        if \"hour\" in posted_text or \"just\" in posted_text or \"minutes\" in posted_text:\n",
    "            posted_date = datetime.now()\n",
    "        elif \"day\" in posted_text:\n",
    "            days_ago = int(posted_text.split()[0])\n",
    "            posted_date = datetime.now() - timedelta(days=days_ago)\n",
    "        else:\n",
    "            posted_date = datetime.now() - timedelta(days=2)  # Assume older if unknown\n",
    "\n",
    "        if posted_date >= cutoff_time:\n",
    "            jobs.append({\n",
    "                \"Source\": \"Indeed\",\n",
    "                \"Title\": title,\n",
    "                \"Company\": company,\n",
    "                \"Location\": location,\n",
    "                \"URL\": link,\n",
    "                \"Posted Date\": posted_date.strftime(\"%Y-%m-%d\"),\n",
    "                \"Date Scraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "\n",
    "    return jobs\n",
    "\n",
    "# Function to scrape LinkedIn dynamically\n",
    "def scrape_linkedin():\n",
    "    driver = setup_driver()\n",
    "    driver.get(\"https://www.linkedin.com/jobs/search/?keywords=Data%20Science&location=United%20States\")\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Scroll down to load more jobs\n",
    "    for _ in range(3):\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(2)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    jobs = []\n",
    "    cutoff_time = datetime.now() - timedelta(days=1)  # 24-hour filter\n",
    "\n",
    "    for job_card in soup.find_all(\"div\", class_=\"base-card\"):\n",
    "        title = job_card.find(\"h3\", class_=\"base-search-card__title\").text.strip() if job_card.find(\"h3\", class_=\"base-search-card__title\") else \"N/A\"\n",
    "        company = job_card.find(\"h4\", class_=\"base-search-card__subtitle\").text.strip() if job_card.find(\"h4\", class_=\"base-search-card__subtitle\") else \"N/A\"\n",
    "        location = job_card.find(\"span\", class_=\"job-search-card__location\").text.strip() if job_card.find(\"span\", class_=\"job-search-card__location\") else \"N/A\"\n",
    "        link = job_card.find(\"a\")[\"href\"] if job_card.find(\"a\") else \"N/A\"\n",
    "        posted_text = job_card.find(\"time\")[\"datetime\"] if job_card.find(\"time\") else \"N/A\"\n",
    "\n",
    "        # Convert posted time to datetime\n",
    "        if posted_text != \"N/A\":\n",
    "            posted_date = datetime.strptime(posted_text, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            posted_date = datetime.now() - timedelta(days=2)  # Assume older if unknown\n",
    "\n",
    "        if posted_date >= cutoff_time:\n",
    "            jobs.append({\n",
    "                \"Source\": \"LinkedIn\",\n",
    "                \"Title\": title,\n",
    "                \"Company\": company,\n",
    "                \"Location\": location,\n",
    "                \"URL\": link,\n",
    "                \"Posted Date\": posted_date.strftime(\"%Y-%m-%d\"),\n",
    "                \"Date Scraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "\n",
    "    return jobs\n",
    "\n",
    "# Function to save jobs to CSV (New jobs at top)\n",
    "def save_to_csv(jobs, filename=\"job_listings.csv\"):\n",
    "    df_new = pd.DataFrame(jobs)\n",
    "\n",
    "    try:\n",
    "        df_existing = pd.read_csv(filename)\n",
    "        df_combined = pd.concat([df_new, df_existing], ignore_index=True)  # New jobs at top\n",
    "    except FileNotFoundError:\n",
    "        df_combined = df_new\n",
    "\n",
    "    df_combined.to_csv(filename, index=False)\n",
    "\n",
    "# Main function with automation condition\n",
    "def main(automation=True):\n",
    "    while True:\n",
    "        print(\"Scraping new jobs...\")\n",
    "\n",
    "        jobs = scrape_indeed() + scrape_linkedin()\n",
    "        if jobs:\n",
    "            save_to_csv(jobs)\n",
    "            print(f\"Scraping completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}. Data saved.\")\n",
    "        else:\n",
    "            print(\"No new jobs found.\")\n",
    "\n",
    "        if not automation:\n",
    "            break  # Run once and exit\n",
    "\n",
    "        time.sleep(43200)  # Run every 12 hours\n",
    "\n",
    "# Run script\n",
    "if __name__ == \"__main__\":\n",
    "    main(automation=False)  # Change to False to run only once\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
