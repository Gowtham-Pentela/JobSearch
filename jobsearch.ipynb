{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping jobs from multiple sources...\n",
      "Scraping completed at 2025-03-17 21:59:44. Data saved.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Setup WebDriver\n",
    "def setup_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in background\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Function to scrape LinkedIn with startup filters\n",
    "def scrape_linkedin_startups():\n",
    "    driver = setup_driver()\n",
    "    url = \"https://www.linkedin.com/jobs/search/?f_C=urn%3Ali%3Afsd_company%3Aunicorns&keywords=Data%20Science&location=United%20States\"\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    for _ in range(3):  # Scroll to load more jobs\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(2)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    jobs = []\n",
    "    cutoff_time = datetime.now() - timedelta(days=1)  # 24-hour filter\n",
    "\n",
    "    for job_card in soup.find_all(\"div\", class_=\"base-card\"):\n",
    "        title = job_card.find(\"h3\").text.strip() if job_card.find(\"h3\") else \"N/A\"\n",
    "        company = job_card.find(\"h4\").text.strip() if job_card.find(\"h4\") else \"N/A\"\n",
    "        location = job_card.find(\"span\", class_=\"job-search-card__location\").text.strip() if job_card.find(\"span\", class_=\"job-search-card__location\") else \"N/A\"\n",
    "        link = job_card.find(\"a\")[\"href\"] if job_card.find(\"a\") else \"N/A\"\n",
    "        posted_text = job_card.find(\"time\")[\"datetime\"] if job_card.find(\"time\") else \"N/A\"\n",
    "\n",
    "        posted_date = datetime.strptime(posted_text, \"%Y-%m-%d\") if posted_text != \"N/A\" else datetime.now() - timedelta(days=2)\n",
    "\n",
    "        if posted_date >= cutoff_time:\n",
    "            jobs.append({\n",
    "                \"Source\": \"LinkedIn - Unicorn Startups\",\n",
    "                \"Title\": title,\n",
    "                \"Company\": company,\n",
    "                \"Location\": location,\n",
    "                \"URL\": link,\n",
    "                \"Posted Date\": posted_date.strftime(\"%Y-%m-%d\"),\n",
    "                \"Date Scraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "\n",
    "    return jobs\n",
    "\n",
    "# Function to scrape Amazon jobs\n",
    "def scrape_amazon():\n",
    "    driver = setup_driver()\n",
    "    driver.get(\"https://www.amazon.jobs/en/search?base_query=Data+Science&loc_query=United+States\")\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    jobs = []\n",
    "    cutoff_time = datetime.now() - timedelta(days=1)\n",
    "\n",
    "    for job_card in soup.find_all(\"div\", class_=\"job\"):\n",
    "        title = job_card.find(\"h3\").text.strip() if job_card.find(\"h3\") else \"N/A\"\n",
    "        location = job_card.find(\"p\", class_=\"location-and-id\").text.strip() if job_card.find(\"p\", class_=\"location-and-id\") else \"N/A\"\n",
    "        link = \"https://www.amazon.jobs\" + job_card.find(\"a\")[\"href\"] if job_card.find(\"a\") else \"N/A\"\n",
    "        posted_date = datetime.now()  # Amazon doesn't show posting date, assuming recent.\n",
    "\n",
    "        if posted_date >= cutoff_time:\n",
    "            jobs.append({\n",
    "                \"Source\": \"Amazon\",\n",
    "                \"Title\": title,\n",
    "                \"Company\": \"Amazon\",\n",
    "                \"Location\": location,\n",
    "                \"URL\": link,\n",
    "                \"Posted Date\": posted_date.strftime(\"%Y-%m-%d\"),\n",
    "                \"Date Scraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "\n",
    "    return jobs\n",
    "\n",
    "# Function to scrape Google jobs\n",
    "def scrape_google():\n",
    "    driver = setup_driver()\n",
    "    driver.get(\"https://careers.google.com/jobs/results/?q=Data%20Science&location=United%20States\")\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    jobs = []\n",
    "    cutoff_time = datetime.now() - timedelta(days=1)\n",
    "\n",
    "    for job_card in soup.find_all(\"li\", class_=\"gc-card\"):\n",
    "        title = job_card.find(\"div\", class_=\"gc-card__title\").text.strip() if job_card.find(\"div\", class_=\"gc-card__title\") else \"N/A\"\n",
    "        location = job_card.find(\"div\", class_=\"gc-card__location\").text.strip() if job_card.find(\"div\", class_=\"gc-card__location\") else \"N/A\"\n",
    "        link = \"https://careers.google.com\" + job_card.find(\"a\")[\"href\"] if job_card.find(\"a\") else \"N/A\"\n",
    "        posted_date = datetime.now()\n",
    "\n",
    "        if posted_date >= cutoff_time:\n",
    "            jobs.append({\n",
    "                \"Source\": \"Google\",\n",
    "                \"Title\": title,\n",
    "                \"Company\": \"Google\",\n",
    "                \"Location\": location,\n",
    "                \"URL\": link,\n",
    "                \"Posted Date\": posted_date.strftime(\"%Y-%m-%d\"),\n",
    "                \"Date Scraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "\n",
    "    return jobs\n",
    "\n",
    "# Function to scrape TCS jobs\n",
    "def scrape_tcs():\n",
    "    driver = setup_driver()\n",
    "    driver.get(\"https://ibegin.tcs.com/iBegin/jobs?keywords=Data%20Science&country=US\")\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    jobs = []\n",
    "    cutoff_time = datetime.now() - timedelta(days=1)\n",
    "\n",
    "    for job_card in soup.find_all(\"div\", class_=\"joblist\"):\n",
    "        title = job_card.find(\"h2\").text.strip() if job_card.find(\"h2\") else \"N/A\"\n",
    "        location = job_card.find(\"p\", class_=\"location\").text.strip() if job_card.find(\"p\", class_=\"location\") else \"N/A\"\n",
    "        link = \"https://ibegin.tcs.com\" + job_card.find(\"a\")[\"href\"] if job_card.find(\"a\") else \"N/A\"\n",
    "        posted_date = datetime.now()\n",
    "\n",
    "        if posted_date >= cutoff_time:\n",
    "            jobs.append({\n",
    "                \"Source\": \"TCS\",\n",
    "                \"Title\": title,\n",
    "                \"Company\": \"TCS\",\n",
    "                \"Location\": location,\n",
    "                \"URL\": link,\n",
    "                \"Posted Date\": posted_date.strftime(\"%Y-%m-%d\"),\n",
    "                \"Date Scraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "\n",
    "    return jobs\n",
    "\n",
    "# Function to save jobs to CSV\n",
    "def save_to_csv(jobs, filename=\"job_listings.csv\"):\n",
    "    df_new = pd.DataFrame(jobs)\n",
    "\n",
    "    try:\n",
    "        df_existing = pd.read_csv(filename)\n",
    "        df_combined = pd.concat([df_new, df_existing], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        df_combined = df_new\n",
    "\n",
    "    df_combined.to_csv(filename, index=False)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    print(\"Scraping jobs from multiple sources...\")\n",
    "    jobs = scrape_amazon() + scrape_google() + scrape_tcs() + scrape_linkedin_startups()\n",
    "    if jobs:\n",
    "        save_to_csv(jobs)\n",
    "        print(f\"Scraping completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}. Data saved.\")\n",
    "    else:\n",
    "        print(\"No new jobs found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping jobs at 2025-03-18 13:16:14...\n",
      "Saved 10 new jobs. Total unique jobs: 10.\n",
      "Scraping completed. 10 new jobs attempted to be added.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import schedule\n",
    "from datetime import datetime, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load config.json for automation toggle\n",
    "def load_config():\n",
    "    try:\n",
    "        with open(\"config.json\", \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {\"automation_enabled\": False}\n",
    "\n",
    "# Save config.json (toggle automation)\n",
    "def save_config(config):\n",
    "    with open(\"config.json\", \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "# Setup WebDriver\n",
    "def setup_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in background\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Function to scrape Amazon jobs\n",
    "def scrape_amazon():\n",
    "    driver = setup_driver()\n",
    "    driver.get(\"https://www.amazon.jobs/en/search?base_query=Data+Science&loc_query=United+States\")\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    jobs = []\n",
    "    cutoff_time = datetime.now() - timedelta(days=1)\n",
    "\n",
    "    for job_card in soup.find_all(\"div\", class_=\"job\"):\n",
    "        title = job_card.find(\"h3\").text.strip() if job_card.find(\"h3\") else \"N/A\"\n",
    "        location = job_card.find(\"p\", class_=\"location-and-id\").text.strip() if job_card.find(\"p\", class_=\"location-and-id\") else \"N/A\"\n",
    "        link = \"https://www.amazon.jobs\" + job_card.find(\"a\")[\"href\"] if job_card.find(\"a\") else \"N/A\"\n",
    "        posted_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        if datetime.now() >= cutoff_time:\n",
    "            jobs.append({\n",
    "                \"Source\": \"Amazon\",\n",
    "                \"Title\": title,\n",
    "                \"Company\": \"Amazon\",\n",
    "                \"Location\": location,\n",
    "                \"URL\": link,\n",
    "                \"Posted Date\": posted_date,\n",
    "                \"Date Scraped\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "\n",
    "    return jobs\n",
    "\n",
    "# Add more functions for Google, TCS, Microsoft, and unicorn startups...\n",
    "\n",
    "# Function to save jobs to CSV (avoid duplicates)\n",
    "def save_to_csv(jobs, filename=\"job_listings.csv\"):\n",
    "    df_new = pd.DataFrame(jobs)\n",
    "\n",
    "    try:\n",
    "        df_existing = pd.read_csv(filename)\n",
    "    except FileNotFoundError:\n",
    "        df_existing = pd.DataFrame(columns=[\"URL\"])  # Empty DataFrame if file doesn't exist\n",
    "\n",
    "    # Remove duplicates based on job URL\n",
    "    df_combined = pd.concat([df_new, df_existing]).drop_duplicates(subset=[\"URL\"], keep=\"first\")\n",
    "\n",
    "    # Save only unique jobs\n",
    "    df_combined.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(df_new)} new jobs. Total unique jobs: {len(df_combined)}.\")\n",
    "\n",
    "# Main function to run scraper\n",
    "def run_scraper():\n",
    "    print(f\"Scraping jobs at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}...\")\n",
    "    jobs = scrape_amazon()  # Add other scrapers here (Google, TCS, Microsoft...)\n",
    "    \n",
    "    if jobs:\n",
    "        save_to_csv(jobs)\n",
    "        print(f\"Scraping completed. {len(jobs)} new jobs attempted to be added.\")\n",
    "    else:\n",
    "        print(\"No new jobs found.\")\n",
    "\n",
    "# Schedule job if automation is enabled\n",
    "def schedule_scraper():\n",
    "    config = load_config()\n",
    "    if config.get(\"automation_enabled\", False):\n",
    "        print(\"Automation enabled. Running scraper every 12 hours.\")\n",
    "        schedule.every(12).hours.do(run_scraper)\n",
    "        \n",
    "        while True:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(60)  # Wait 1 min before checking the schedule again\n",
    "\n",
    "# Enable automation manually\n",
    "def enable_automation():\n",
    "    config = load_config()\n",
    "    config[\"automation_enabled\"] = True\n",
    "    save_config(config)\n",
    "    print(\"Automation enabled. The scraper will run every 12 hours.\")\n",
    "\n",
    "# Disable automation manually\n",
    "def disable_automation():\n",
    "    config = load_config()\n",
    "    config[\"automation_enabled\"] = False\n",
    "    save_config(config)\n",
    "    print(\"Automation disabled.\")\n",
    "\n",
    "# Start the script\n",
    "if __name__ == \"__main__\":\n",
    "    config = load_config()\n",
    "    \n",
    "    if config.get(\"automation_enabled\", False):\n",
    "        schedule_scraper()\n",
    "    else:\n",
    "        run_scraper()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
